#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ì˜¬ë°”ë¥¸ ì–´ë·°ì§• ìŠ¤ì½”ì–´ë§ - 10ê°œ ë¡œì§ ì¢…í•©, ë§¤ì²´ì‚¬ ê·œëª¨ë³„ ì°¨ë³„í™”"""

import pandas as pd
import numpy as np
import yaml
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def proper_scoring():
    """ì˜¬ë°”ë¥¸ ì–´ë·°ì§• ìŠ¤ì½”ì–´ë§"""
    logger.info("ğŸ¯ ì˜¬ë°”ë¥¸ ì¢…í•© ì–´ë·°ì§• ìŠ¤ì½”ì–´ë§ ì‹œì‘...")
    
    # ì„¤ì • ë¡œë“œ
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    logic_weights = config['logic_weights']
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv('qqqq/df_join_abuse.csv')
    abuse_cols = [f'abuse_{i}' for i in range(1, 11) if f'abuse_{i}' in df.columns]
    
    logger.info(f"ë°ì´í„°: {len(df):,}í–‰, ë¡œì§: {len(abuse_cols)}ê°œ")
    
    # 1ë‹¨ê³„: ë§¤ì²´ì‚¬ ê·œëª¨ ë¶„ë¥˜
    logger.info("ë§¤ì²´ì‚¬ ê·œëª¨ ë¶„ë¥˜...")
    
    mda_traffic = df.groupby('mda_idx').size().sort_values(ascending=False)
    
    # ê·œëª¨ë³„ ë¶„ë¥˜ (íŠ¸ë˜í”½ ê¸°ì¤€)
    large_threshold = mda_traffic.quantile(0.9)    # ìƒìœ„ 10% = ëŒ€í˜•
    medium_threshold = mda_traffic.quantile(0.7)   # ìƒìœ„ 30% = ì¤‘í˜•
    
    large_mdas = set(mda_traffic[mda_traffic >= large_threshold].index)
    medium_mdas = set(mda_traffic[(mda_traffic >= medium_threshold) & (mda_traffic < large_threshold)].index)
    small_mdas = set(mda_traffic[mda_traffic < medium_threshold].index)
    
    logger.info(f"ëŒ€í˜•: {len(large_mdas)}ê°œ, ì¤‘í˜•: {len(medium_mdas)}ê°œ, ì†Œí˜•: {len(small_mdas)}ê°œ")
    
    # 2ë‹¨ê³„: ë§¤ì²´ì‚¬ë³„ ì¢…í•© ìŠ¤ì½”ì–´ë§
    logger.info("ë§¤ì²´ì‚¬ë³„ ì¢…í•© ìŠ¤ì½”ì–´ë§...")
    
    mda_results = []
    
    for mda_id in df['mda_idx'].unique():
        if pd.isna(mda_id):
            continue
            
        mda_data = df[df['mda_idx'] == mda_id]
        total_traffic = len(mda_data)
        
        # ê·œëª¨ ë¶„ë¥˜
        if mda_id in large_mdas:
            mda_size = 'LARGE'
            size_factor = 1.2  # ëŒ€í˜•ì€ ê¸°ì¤€ ê°•í™”
        elif mda_id in medium_mdas:
            mda_size = 'MEDIUM' 
            size_factor = 1.0  # ì¤‘í˜•ì€ ê¸°ì¤€
        else:
            mda_size = 'SMALL'
            size_factor = 0.8  # ì†Œí˜•ì€ ê¸°ì¤€ ì™„í™”
        
        total_score = 0.0
        logic_scores = {}
        logic_details = {}
        
        # 3ë‹¨ê³„: ë¡œì§ë³„ ì ìˆ˜ ê³„ì‚° (10ê°œ ëª¨ë‘)
        for col in abuse_cols:
            if col not in mda_data.columns:
                continue
                
            logic_num = int(col.split('_')[1])
            if logic_num not in logic_weights:
                continue
            
            abuse_data = mda_data[mda_data[col] > 0]
            if len(abuse_data) == 0:
                continue
            
            # ê¸°ë³¸ ì§€í‘œ
            frequency = len(abuse_data)
            severity = abuse_data[col].mean()
            abuse_rate = frequency / total_traffic
            
            # ê·œëª¨ë³„ ì¡°ì •ëœ magnitude ê³„ì‚°
            if mda_size == 'LARGE':
                # ëŒ€í˜•: ë¹„ìœ¨ ì¤‘ì‹¬, ì ˆëŒ€ê°’ ì–µì œ
                magnitude = (abuse_rate ** 0.9) * (np.log1p(frequency) ** 0.2) * severity
            elif mda_size == 'MEDIUM':
                # ì¤‘í˜•: ê· í˜•
                magnitude = (abuse_rate ** 0.8) * (np.log1p(frequency) ** 0.3) * severity  
            else:
                # ì†Œí˜•: ì ˆëŒ€ê°’ë„ ê³ ë ¤
                magnitude = (abuse_rate ** 0.7) * (np.log1p(frequency) ** 0.4) * severity
            
            # 4ë‹¨ê³„: ê·¹ê°’ ì¡°ì ˆ (tanh í¬í™”í•¨ìˆ˜)
            normalized_mag = np.tanh(magnitude * 3)  # 0~1 ì‚¬ì´ë¡œ ì¡°ì ˆ
            
            # 5ë‹¨ê³„: ìµœì¢… ë¡œì§ ì ìˆ˜
            base_weight = logic_weights[logic_num]
            recency_boost = 1.1 if logic_num in [1, 3, 6, 7, 8] else 1.0
            
            logic_score = base_weight * normalized_mag * recency_boost * size_factor
            
            total_score += logic_score
            logic_scores[logic_num] = logic_score
            logic_details[logic_num] = {
                'rate': abuse_rate,
                'frequency': frequency,
                'severity': severity,
                'magnitude': magnitude,
                'normalized': normalized_mag
            }
        
        if total_score > 0:
            # ìƒìœ„ ê¸°ì—¬ ë¡œì§
            top_contribs = sorted(logic_scores.items(), key=lambda x: x[1], reverse=True)[:3]
            contrib_str = ', '.join([f"L{lid}:{score:.3f}" for lid, score in top_contribs])
            
            mda_results.append({
                'entity_id': mda_id,
                'total_score': total_score,
                'size_category': mda_size,
                'traffic_size': total_traffic,
                'logic_count': len(logic_scores),
                'contributions': contrib_str,
                'top_logic': max(logic_scores.items(), key=lambda x: x[1])[0] if logic_scores else None,
                'score_balance': len([s for s in logic_scores.values() if s > total_score * 0.1])  # 10% ì´ìƒ ê¸°ì—¬í•˜ëŠ” ë¡œì§ ìˆ˜
            })
    
    # ê²°ê³¼ ì •ë ¬
    mda_df = pd.DataFrame(mda_results).sort_values('total_score', ascending=False).reset_index(drop=True)
    
    logger.info(f"ë§¤ì²´ì‚¬ ì™„ë£Œ: {len(mda_df)}ê°œ")
    
    # ì „ì—­ ë³€ìˆ˜ ì €ì¥
    globals()['mda_scores'] = mda_df
    
    # í¼ë¸”ë¦¬ì…” ìŠ¤ì½”ì–´ë§ (ê°„ë‹¨í™”)
    logger.info("í¼ë¸”ë¦¬ì…” ìŠ¤ì½”ì–´ë§...")
    pub_results = []
    pub_sample = df.groupby('pub_sub_rel_id').apply(lambda x: x.head(1000)).reset_index(drop=True)
    
    for pub_id in pub_sample['pub_sub_rel_id'].value_counts().head(1000).index:
        if pd.isna(pub_id):
            continue
        pub_data = pub_sample[pub_sample['pub_sub_rel_id'] == pub_id]
        parent_mda = pub_data['mda_idx'].mode()[0] if len(pub_data['mda_idx'].mode()) > 0 else 'unknown'
        
        total_score = 0.0
        for col in abuse_cols:
            if col not in pub_data.columns:
                continue
            logic_num = int(col.split('_')[1])
            if logic_num not in logic_weights:
                continue
            
            abuse_data = pub_data[pub_data[col] > 0]
            if len(abuse_data) == 0:
                continue
                
            abuse_rate = len(abuse_data) / len(pub_data)
            severity = abuse_data[col].mean()
            magnitude = (abuse_rate ** 0.8) * (np.log1p(len(abuse_data)) ** 0.3) * severity
            normalized_mag = np.tanh(magnitude * 2)
            
            logic_score = logic_weights[logic_num] * normalized_mag * 0.8  # í¼ë¸”ë¦¬ì…” í• ì¸
            total_score += logic_score
        
        if total_score > 0:
            pub_results.append({
                'entity_id': pub_id,
                'parent_mda': parent_mda,
                'total_score': total_score
            })
    
    pub_df = pd.DataFrame(pub_results).sort_values('total_score', ascending=False).reset_index(drop=True)
    logger.info(f"í¼ë¸”ë¦¬ì…” ì™„ë£Œ: {len(pub_df)}ê°œ")
    
    # ì‚¬ìš©ì ìŠ¤ì½”ì–´ë§ (ìƒ˜í”Œë§)
    logger.info("ì‚¬ìš©ì ìŠ¤ì½”ì–´ë§...")
    user_sample = df.sample(n=min(100000, len(df)), random_state=42)
    user_sample = user_sample.copy()
    user_sample['user_key'] = user_sample.apply(lambda row: 
        f"dvc:{int(row['dvc_idx'])}" if pd.notna(row['dvc_idx']) and row['dvc_idx'] != 0 
        else f"ip:{row['user_ip']}" if pd.notna(row['user_ip']) 
        else 'unknown', axis=1)
    
    user_results = []
    for user_key in user_sample['user_key'].value_counts().head(3000).index:
        if user_key == 'unknown':
            continue
        user_data = user_sample[user_sample['user_key'] == user_key]
        
        total_score = 0.0
        for col in abuse_cols:
            if col not in user_data.columns:
                continue
            logic_num = int(col.split('_')[1])
            if logic_num not in logic_weights:
                continue
                
            abuse_data = user_data[user_data[col] > 0]
            if len(abuse_data) == 0:
                continue
                
            abuse_rate = len(abuse_data) / len(user_data)
            severity = abuse_data[col].mean()
            magnitude = (abuse_rate ** 0.85) * (np.log1p(len(abuse_data)) ** 0.25) * severity
            normalized_mag = np.tanh(magnitude * 2)
            
            logic_score = logic_weights[logic_num] * normalized_mag * 0.6  # ì‚¬ìš©ì í• ì¸
            total_score += logic_score
        
        if total_score > 0:
            user_results.append({
                'entity_id': user_key,
                'total_score': total_score
            })
    
    user_df = pd.DataFrame(user_results).sort_values('total_score', ascending=False).reset_index(drop=True)
    logger.info(f"ì‚¬ìš©ì ì™„ë£Œ: {len(user_df)}ê°œ")
    
    # ì¢…í•© í†µí•©
    overall_list = []
    overall_list.append(mda_df.head(50).assign(dimension='MDA', normalized_score=lambda x: x['total_score'] * 1.0))
    overall_list.append(pub_df.head(100).assign(dimension='PUB', normalized_score=lambda x: x['total_score'] * 0.8))
    overall_list.append(user_df.head(100).assign(dimension='USER', normalized_score=lambda x: x['total_score'] * 0.6))
    
    overall_df = pd.concat([df[['entity_id', 'dimension', 'normalized_score', 'total_score']] for df in overall_list], ignore_index=True)
    overall_df = overall_df.sort_values('normalized_score', ascending=False).reset_index(drop=True)
    overall_df['rank'] = range(1, len(overall_df) + 1)
    
    # ì „ì—­ ë³€ìˆ˜ ì €ì¥
    globals()['mda_scores'] = mda_df
    globals()['pub_scores'] = pub_df  
    globals()['user_scores'] = user_df
    globals()['overall_scores'] = overall_df
    
    logger.info("âœ… ì˜¬ë°”ë¥¸ ì¢…í•© ìŠ¤ì½”ì–´ë§ ì™„ë£Œ!")
    
    return mda_df, pub_df, user_df, overall_df

if __name__ == "__main__":
    mda_scores, pub_scores, user_scores, overall_scores = proper_scoring()
    
    print("\nğŸ† ì˜¬ë°”ë¥¸ TOP 15 ë§¤ì²´ì‚¬:")
    print(mda_scores[['entity_id', 'total_score', 'size_category', 'traffic_size', 'logic_count', 'score_balance', 'contributions']].head(15))
    
    # 539ë²ˆ ë¶„ì„
    if 539 in mda_scores['entity_id'].values:
        mda_539 = mda_scores[mda_scores['entity_id']==539].iloc[0]
        rank_539 = mda_scores[mda_scores['entity_id']==539].index[0] + 1
        print(f"\nğŸ” 539ë²ˆ ìƒì„¸ ë¶„ì„:")
        print(f"ìˆœìœ„: {rank_539}")
        print(f"ì ìˆ˜: {mda_539['total_score']:.3f}")
        print(f"ê·œëª¨: {mda_539['size_category']}")
        print(f"ê¸°ì—¬ ë¡œì§ìˆ˜: {mda_539['logic_count']}ê°œ")
        print(f"ê· í˜• ë¡œì§ìˆ˜: {mda_539['score_balance']}ê°œ")
        print(f"ê¸°ì—¬ë„: {mda_539['contributions']}")
    
    # ê·œëª¨ë³„ ë¶„í¬
    print(f"\nğŸ“Š TOP 15 ê·œëª¨ë³„ ë¶„í¬:")
    size_dist = mda_scores.head(15)['size_category'].value_counts()
    for size, count in size_dist.items():
        print(f"{size}: {count}ê°œ")
    
    print(f"\nğŸ“š TOP 10 í¼ë¸”ë¦¬ì…”:")
    print(pub_scores[['entity_id', 'parent_mda', 'total_score']].head(10))
    
    print(f"\nğŸ“± TOP 10 ì‚¬ìš©ì:")
    print(user_scores[['entity_id', 'total_score']].head(10))
    
    print(f"\nğŸ† ì¢…í•© TOP 20:")
    print(overall_scores[['rank', 'dimension', 'entity_id', 'normalized_score']].head(20))
    
    print(f"\nğŸ¯ ìŠ¤ì½”ì–´ë§ íŠ¹ì§•:")
    print("âœ… 10ê°œ ë¡œì§ ëª¨ë‘ ì¢…í•© í‰ê°€")
    print("âœ… ëŒ€í˜•/ì¤‘í˜•/ì†Œí˜• ë§¤ì²´ì‚¬ë³„ ì°¨ë³„í™”ëœ ê¸°ì¤€")  
    print("âœ… ê·¹ê°’ì€ tanh í•¨ìˆ˜ë¡œ ì ì ˆíˆ ì¡°ì ˆ")
    print("âœ… í•œ ë¡œì§ í¸í–¥ ë°©ì§€ (balance ì ìˆ˜)")
    print("âœ… 4ê°œ ì°¨ì› ë°ì´í„°í”„ë ˆì„ ì™„ì„±")
